{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYiEwBcp2XpRfmNwB+8kmZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kirstenpamaran/ISYS2001_Assignment/blob/main/Report_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enron"
      ],
      "metadata": {
        "id": "kQoJ_aEF5yZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Preparation of data was done using sqlite3 and pandas.\n",
        "All tasks were completed and visualised graphs can be displayed in the project.\n",
        "\n",
        "## Email traffic over time\n",
        "Data for messages were collected required to be validated by the date in order to group them by year.\n",
        "The date column for the messages dataframe had to be converted to pandas date and remove any null or invalid date formats. \n",
        "\n",
        "Assumptions to be made: \n",
        "- max year was found: 2044 assume valid\n",
        "\n",
        "No issues found here\n",
        "\n",
        "## Top senders and recipients\n",
        "2 horizontal charts to be visualised, one for top senders and the other for top recipients. Limitations is that the more data displayed on the chart, the more difficult it is to have a better visualisation as the text becomes smaller and compact to one another. \n",
        "No issues found here\n",
        "\n",
        "\n",
        "## Email distribution by recipient type\n",
        "The total count of 'to' 'cc' 'bcc' in the recipientinfo dataframe, using value_counts on the column 'rtype'  would return the result of all the counts for the recipient type. \n",
        "No issues found here\n",
        "\n",
        "## Subject keyword analysis\n",
        "Data had to be cleaned using the function provided in the worksheets, the rows in the subject column had invalid characters that would not be processed in the wordcloud. Dat acleaning process used to filter out the invalid characters using regular expression before generating a wordcloud chart with the subject data.\n",
        "No issues found here\n",
        "\n",
        "\n",
        "## Internal vs. External Communication\n",
        "Using the pandas function merge() to merge two datafgrames together by mid and isin to filter out the external emails from the messages dataframe in order to find the amount of internal communication occurring in the enron's database. External information is found using the ~isin() function andthe or '|' to determine whether an employee email has been communicating with external emails. Visualisation graph is displayed to show that there are more external communication activities occuring than internal\n",
        "\n",
        "No issues found here\n",
        "\n",
        "\n",
        "\n",
        "##Conclusion\n",
        "\n",
        "Improvements can be done for data cleaning as the data was not initially clean, rows and values contained multiple blanks and invalid values such as the date and emails. Data cleaning was not done in one step but done over through all the cells. The severity of this problem is nothing major but more of a hassle as you would have to consistently validate and clean the data before performing any filtering and grouping instead of validating it in one single step. This issue can be fixed by initially cleaning the data in one code blcok one loaded the dataframes before performing any grouping. If performed data cleaning initially at the beginning would produce more accurate findings as the data between dataframes had to be cross-validated such as the emails, message id, mids."
      ],
      "metadata": {
        "id": "Ce-zClZI6C30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Code"
      ],
      "metadata": {
        "id": "hZU7oDqr7xA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message_count = m_df.groupby(m_df['date'].dt.date).size().reset_index(name='count')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "pwL8nscH8Poz",
        "outputId": "04b669eb-68a1-4f43-9fcd-d2e58f558ca1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3fbe4552216b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmessage_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'm_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# filter dataframe using isin() to retrieve rows with the sender and rvalue matching any emails in the employeelist \n",
        "df_internal_comm = df_merged[df_merged['sender'].isin(employee_df['Email_id']) & df_merged['rvalue'].isin(employee_df['Email_id'])]\n",
        "\n",
        "# filter dataframe using ~isin() to retrieve rows with either sender or recipient \n",
        "# in the employeelist. Only one has to match for it to be an external comm\n",
        "df_external_comm = df_merged[~df_merged['sender'].isin(employee_df['Email_id']) | ~df_merged['rvalue'].isin(employee_df['Email_id'])]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "pA__yFn58rsI",
        "outputId": "789cfc5b-eb29-4025-b750-1734fc2e8621"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-bde42ab244c1>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# filter dataframe using isin() to retrieve rows with the sender and rvalue matching any emails in the employeelist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_internal_comm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_merged\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_merged\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sender'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memployee_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Email_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mdf_merged\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rvalue'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memployee_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Email_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# filter dataframe using ~isin() to retrieve rows with either sender or recipient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# in the employeelist. Only one has to match for it to be an external comm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_merged' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for each row, count each messages a sender has sent and sort them by descending value\n",
        "sender_count = m_df['sender'].value_counts().sort_values(ascending=False).reset_index()\n",
        "# for each row, count each messages each recipient received and sort them by descending value\n",
        "recipient_count = recipientinfo_df['rvalue'].value_counts().sort_values(ascending=False).reset_index()\n",
        "\n"
      ],
      "metadata": {
        "id": "4oipibnY8rzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_df = m_df[(m_df['date'].notnull())]"
      ],
      "metadata": {
        "id": "Ju_eSo-Q8r6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_df = message_df\n",
        "#\n",
        "m_df['date'] = pd.to_datetime(m_df['date'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
        "min_year = m_df['date'].dt.year.min()\n",
        "max_year = m_df['date'].dt.year.max()\n",
        "\n",
        "\n",
        "print(message_df.shape[0])\n",
        "\n",
        "m_df = m_df[(m_df['date'].notnull())]\n",
        "\n",
        "\n",
        "\n",
        "print('Min year: ' + str(min_year))\n",
        "print('Max year: ' + str(max_year))\n",
        "print('Length of dataframe ' + str(m_df.shape[0]))\n"
      ],
      "metadata": {
        "id": "odIPrjXqtHfG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "1addcb4f-4910-46c0-cb80-8882a509a930"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-6a6ee6b6ecca>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mm_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%Y-%m-%d %H:%M:%S'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coerce'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmin_year\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmax_year\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'message_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(message_df.shape[0])\n",
        "\n",
        "m_df = m_df[(m_df['date'].notnull())]\n",
        "\n",
        "print('Min year: ' + str(min_year))\n",
        "print('Max year: ' + str(max_year))\n",
        "print('Length of dataframe ' + str(m_df.shape[0]))"
      ],
      "metadata": {
        "id": "x2zYcMoF8wq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "con = sqlite3.connect('enron.db')\n",
        "cur = con.cursor()"
      ],
      "metadata": {
        "id": "iA-wekR99cSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ENGLISH_STOP_WORDS = set([\n",
        "    'a',\n",
        "    'about',\n",
        "    'above',\n",
        "    'across',\n",
        "    'after',\n",
        "    'afterwards',\n",
        "    'again',\n",
        "    'against',\n",
        "    'ain',\n",
        "    'all',\n",
        "    'almost',\n",
        "    'alone',\n",
        "    'along',\n",
        "    'already',\n",
        "    'also',\n",
        "    'although',\n",
        "    'always',\n",
        "    'am',\n",
        "    'among',\n",
        "    'amongst',\n",
        "    'amoungst',\n",
        "    'amount',\n",
        "    'an',\n",
        "    'and',\n",
        "    'another',\n",
        "    'any',\n",
        "    'anyhow',\n",
        "    'anyone',\n",
        "    'anything',\n",
        "    'anyway',\n",
        "    'anywhere',\n",
        "    'are',\n",
        "    'aren',\n",
        "    'around',\n",
        "    'as',\n",
        "    'at',\n",
        "    'back',\n",
        "    'be',\n",
        "    'became',\n",
        "    'because',\n",
        "    'become',\n",
        "    'becomes',\n",
        "    'becoming',\n",
        "    'been',\n",
        "    'before',\n",
        "    'beforehand',\n",
        "    'behind',\n",
        "    'being',\n",
        "    'below',\n",
        "    'beside',\n",
        "    'besides',\n",
        "    'between',\n",
        "    'beyond',\n",
        "    'bill',\n",
        "    'both',\n",
        "    'bottom',\n",
        "    'but',\n",
        "    'by',\n",
        "    'call',\n",
        "    'can',\n",
        "    'cannot',\n",
        "    'cant',\n",
        "    'co',\n",
        "    'con',\n",
        "    'could',\n",
        "    'couldn',\n",
        "    'couldnt',\n",
        "    'cry',\n",
        "    'd',\n",
        "    'de',\n",
        "    'describe',\n",
        "    'detail',\n",
        "    'did',\n",
        "    'didn',\n",
        "    'do',\n",
        "    'does',\n",
        "    'doesn',\n",
        "    'doing',\n",
        "    'don',\n",
        "    'done',\n",
        "    'down',\n",
        "    'due',\n",
        "    'during',\n",
        "    'each',\n",
        "    'eg',\n",
        "    'eight',\n",
        "    'either',\n",
        "    'eleven',\n",
        "    'else',\n",
        "    'elsewhere',\n",
        "    'empty',\n",
        "    'enough',\n",
        "    'etc',\n",
        "    'even',\n",
        "    'ever',\n",
        "    'every',\n",
        "    'everyone',\n",
        "    'everything',\n",
        "    'everywhere',\n",
        "    'except',\n",
        "    'few',\n",
        "    'fifteen',\n",
        "    'fify',\n",
        "    'fill',\n",
        "    'find',\n",
        "    'fire',\n",
        "    'first',\n",
        "    'five',\n",
        "    'for',\n",
        "    'former',\n",
        "    'formerly',\n",
        "    'forty',\n",
        "    'found',\n",
        "    'four',\n",
        "    'from',\n",
        "    'front',\n",
        "    'full',\n",
        "    'further',\n",
        "    'get',\n",
        "    'give',\n",
        "    'go',\n",
        "    'had',\n",
        "    'hadn',\n",
        "    'has',\n",
        "    'hasn',\n",
        "    'hasnt',\n",
        "    'have',\n",
        "    'haven',\n",
        "    'having',\n",
        "    'he',\n",
        "    'hence',\n",
        "    'her',\n",
        "    'here',\n",
        "    'hereafter',\n",
        "    'hereby',\n",
        "    'herein',\n",
        "    'hereupon',\n",
        "    'hers',\n",
        "    'herself',\n",
        "    'him',\n",
        "    'himself',\n",
        "    'his',\n",
        "    'how',\n",
        "    'however',\n",
        "    'hundred',\n",
        "    'i',\n",
        "    'ie',\n",
        "    'if',\n",
        "    'in',\n",
        "    'inc',\n",
        "    'indeed',\n",
        "    'interest',\n",
        "    'into',\n",
        "    'is',\n",
        "    'isn',\n",
        "    'it',\n",
        "    'its',\n",
        "    'itself',\n",
        "    'just',\n",
        "    'keep',\n",
        "    'last',\n",
        "    'latter',\n",
        "    'latterly',\n",
        "    'least',\n",
        "    'less',\n",
        "    'll',\n",
        "    'ltd',\n",
        "    'm',\n",
        "    'ma',\n",
        "    'made',\n",
        "    'many',\n",
        "    'may',\n",
        "    'me',\n",
        "    'meanwhile',\n",
        "    'might',\n",
        "    'mightn',\n",
        "    'mill',\n",
        "    'mine',\n",
        "    'more',\n",
        "    'moreover',\n",
        "    'most',\n",
        "    'mostly',\n",
        "    'move',\n",
        "    'much',\n",
        "    'must',\n",
        "    'mustn',\n",
        "    'my',\n",
        "    'myself',\n",
        "    'name',\n",
        "    'namely',\n",
        "    'needn',\n",
        "    'neither',\n",
        "    'never',\n",
        "    'nevertheless',\n",
        "    'next',\n",
        "    'nine',\n",
        "    'no',\n",
        "    'nobody',\n",
        "    'none',\n",
        "    'noone',\n",
        "    'nor',\n",
        "    'not',\n",
        "    'nothing',\n",
        "    'now',\n",
        "    'nowhere',\n",
        "    'o',\n",
        "    'of',\n",
        "    'off',\n",
        "    'often',\n",
        "    'on',\n",
        "    'once',\n",
        "    'one',\n",
        "    'only',\n",
        "    'onto',\n",
        "    'or',\n",
        "    'other',\n",
        "    'others',\n",
        "    'otherwise',\n",
        "    'our',\n",
        "    'ours',\n",
        "    'ourselves',\n",
        "    'out',\n",
        "    'over',\n",
        "    'own',\n",
        "    'part',\n",
        "    'per',\n",
        "    'perhaps',\n",
        "    'please',\n",
        "    'put',\n",
        "    'rather',\n",
        "    're',\n",
        "    's',\n",
        "    'same',\n",
        "    'see',\n",
        "    'seem',\n",
        "    'seemed',\n",
        "    'seeming',\n",
        "    'seems',\n",
        "    'serious',\n",
        "    'several',\n",
        "    'shan',\n",
        "    'she',\n",
        "    'should',\n",
        "    'shouldn',\n",
        "    'show',\n",
        "    'side',\n",
        "    'since',\n",
        "    'sincere',\n",
        "    'six',\n",
        "    'sixty',\n",
        "    'so',\n",
        "    'some',\n",
        "    'somehow',\n",
        "    'someone',\n",
        "    'something',\n",
        "    'sometime',\n",
        "    'sometimes',\n",
        "    'somewhere',\n",
        "    'still',\n",
        "    'such',\n",
        "    'system',\n",
        "    't',\n",
        "    'take',\n",
        "    'ten',\n",
        "    'than',\n",
        "    'that',\n",
        "    'the',\n",
        "    'their',\n",
        "    'theirs',\n",
        "    'them',\n",
        "    'themselves',\n",
        "    'then',\n",
        "    'thence',\n",
        "    'there',\n",
        "    'thereafter',\n",
        "    'thereby',\n",
        "    'therefore',\n",
        "    'therein',\n",
        "    'thereupon',\n",
        "    'these',\n",
        "    'they',\n",
        "    'thick',\n",
        "    'thin',\n",
        "    'third',\n",
        "    'this',\n",
        "    'those',\n",
        "    'though',\n",
        "    'three',\n",
        "    'through',\n",
        "    'throughout',\n",
        "    'thru',\n",
        "    'thus',\n",
        "    'to',\n",
        "    'together',\n",
        "    'too',\n",
        "    'top',\n",
        "    'toward',\n",
        "    'towards',\n",
        "    'twelve',\n",
        "    'twenty',\n",
        "    'two',\n",
        "    'un',\n",
        "    'under',\n",
        "    'until',\n",
        "    'up',\n",
        "    'upon',\n",
        "    'us',\n",
        "    've',\n",
        "    'very',\n",
        "    'via',\n",
        "    'was',\n",
        "    'wasn',\n",
        "    'we',\n",
        "    'well',\n",
        "    'were',\n",
        "    'weren',\n",
        "    'what',\n",
        "    'whatever',\n",
        "    'when',\n",
        "    'whence',\n",
        "    'whenever',\n",
        "    'where',\n",
        "    'whereafter',\n",
        "    'whereas',\n",
        "    'whereby',\n",
        "    'wherein',\n",
        "    'whereupon',\n",
        "    'wherever',\n",
        "    'whether',\n",
        "    'which',\n",
        "    'while',\n",
        "    'whither',\n",
        "    'who',\n",
        "    'whoever',\n",
        "    'whole',\n",
        "    'whom',\n",
        "    'whose',\n",
        "    'why',\n",
        "    'will',\n",
        "    'with',\n",
        "    'within',\n",
        "    'without',\n",
        "    'won',\n",
        "    'would',\n",
        "    'wouldn',\n",
        "    'y',\n",
        "    'yet',\n",
        "    'you',\n",
        "    'your',\n",
        "    'yours',\n",
        "    'yourself',\n",
        "    'yourselves'\n",
        "])"
      ],
      "metadata": {
        "id": "i9HOx0kBGG1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(text):\n",
        "  ''' Uses regular expresison to extract english letter and digits from the supplied text. '''\n",
        "  regExp = \"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\"\n",
        "  return ' '.join(re.sub(regExp, \" \", text).split())"
      ],
      "metadata": {
        "id": "NSNksEXaGZz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge two dataframes from messages and recipientinfo table by message id\n",
        "df_merged = m_df.merge(recipientinfo_df, on='mid')\n",
        "\n",
        "# filter dataframe using isin() to retrieve rows with the sender and rvalue matching any emails in the employeelist \n",
        "df_internal_comm = df_merged[df_merged['sender'].isin(employee_df['Email_id']) & df_merged['rvalue'].isin(employee_df['Email_id'])]\n",
        "\n",
        "# filter dataframe using ~isin() to retrieve rows with either sender or recipient \n",
        "# in the employeelist. Only one has to match for it to be an external comm\n",
        "df_external_comm = df_merged[~df_merged['sender'].isin(employee_df['Email_id']) | ~df_merged['rvalue'].isin(employee_df['Email_id'])]\n",
        "\n",
        "# check to see the number of internal and external emails\n",
        "# make sure the internal + external emails add up to the total count df_merged rows\n",
        "print(df_merged.shape[0])\n",
        "print(df_internal_comm.shape[0])\n",
        "print(df_external_comm.shape[0])\n",
        "\n",
        "# total internal and external sum has to be equal to the number of mids \n",
        "assert df_internal_comm.shape[0] + df_external_comm.shape[0] == df_merged.shape[0], \"'The sum of internal and external emails should be equal to the number of emails in the df_merged list'\"\n",
        "\n",
        "# Plot our result on a pie graph\n",
        "label = ['Internal Comms', 'External Comms']\n",
        "values = [df_internal_comm.shape[0],df_external_comm.shape[0]]\n",
        "plt.pie(values,labels=label)\n",
        "plt.title('Internal Communication vs External Communication between Employees')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EfLrcj-9HWTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Get all the email subjects and apply clean() retrieve only human readable text\n",
        "df_clean = pd.DataFrame(columns=['Clean text'])\n",
        "df_clean['Clean text'] = df_m['subject'].apply(clean)\n",
        "\n",
        "# Get all the messages for each clean text and merge them into one string object for wordcloud\n",
        "messages = ' '.join(df_clean['Clean text'])\n",
        "wordcloud = WordCloud(width=680, height=480, margin=0,stopwords=ENGLISH_STOP_WORDS).generate(messages)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.margins(x=0, y=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wpJfXpKqGh3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "\n",
        "\n",
        "[1] https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html\n",
        "\n",
        "[2] https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isin.html\n",
        "\n",
        "[3] https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html\n",
        "\n",
        "[4] https://pandas.pydata.org/docs/reference/api/pandas.notnull.html\n",
        "\n",
        "[5] https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html\n",
        "\n",
        "[6] https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shape.html\n",
        "\n",
        "[7] https://docs.python.org/3/library/sqlite3.html\n",
        "\n",
        "[8] https://raw.githubusercontent.com/michael-borck/isys2001-worksheets/main/stopwords.py \n",
        "\n",
        "[9] https://github.com/michael-borck/isys2001-worksheets/blob/main/twitter_data_cleaning.ipynb\n",
        "\n",
        "[10] https://github.com/michael-borck/isys2001-worksheets/blob/main/visualise_twitter.ipynb\n",
        "\n",
        "[11] https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html and https://towardsdatascience.com/pandas-in-notin-ff2415f1e3e1 \n"
      ],
      "metadata": {
        "id": "yRlSh5f75jBb"
      }
    }
  ]
}